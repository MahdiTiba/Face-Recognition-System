{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b2cafa",
   "metadata": {},
   "source": [
    "### Requirement Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "33d409fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52036bd",
   "metadata": {},
   "source": [
    "### Creating Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f95221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "986d9521",
   "metadata": {},
   "source": [
    "### Build pair of images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dffd26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_array(root_path, img_target_size=(300, 300), numeric_labels=True):\n",
    "    \"\"\"\n",
    "    read images from directories as a numpy array.\n",
    "    \n",
    "    :param root_path(str): root path refering to the path containing sub-folders\n",
    "    :param img_target_size(tuple): a tuple with size(W, L) to convert image sizes\n",
    "    :param numeric_labels: if true, output labels will be int. if false, the folder names considered as label\n",
    "    :return img_array: a numpy array containing all image with shape (#images, img_target_size, #channels)\n",
    "    \"\"\"\n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    class_names = os.listdir(root_path)\n",
    "    #class_names = [str(i) for i in sorted([int(j) for j in os.listdir(root_path)])]\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        img_names = os.listdir(os.path.join(root_path, class_name))\n",
    "\n",
    "        for img_name in img_names:\n",
    "            path = os.path.join(root_path, class_name, img_name)\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, img_target_size)\n",
    "            img_list.append(img)\n",
    "            if numeric_labels:\n",
    "                label_list.append(i)\n",
    "            else:\n",
    "                label_list.append(class_name)\n",
    "        \n",
    "    img_array = np.array(img_list)\n",
    "    label_array = np.array(label_list)\n",
    "    \n",
    "    return img_array, label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "86585f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_array.shape : (152, 300, 300, 3)\n",
      "label_array.shape : (152,)\n"
     ]
    }
   ],
   "source": [
    "root_path = 'C:\\\\Users\\Lenovo\\Documents\\Github\\Datasets\\Facial-Emotion-Recognition\\\\Data'\n",
    "img_array, label_array = images_to_array(root_path)\n",
    "print(f'img_array.shape : {img_array.shape}')\n",
    "print(f'label_array.shape : {label_array.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c32daa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(images, labels):\n",
    "    # initialize two empty lists to hold the (image, image) pairs and\n",
    "    # labels to indicate if a pair is positive or negative\n",
    "    pair_images = []\n",
    "    pair_labels = []\n",
    "    \n",
    "    # calculate the total number of classes present in the dataset\n",
    "    # and then build a list of indexes for each class label that\n",
    "    # provides the indexes for all examples with a given label\n",
    "    num_classes = len(np.unique(labels))\n",
    "    idx = [np.where(labels == i)[0] for i in range(0, num_classes)] \n",
    "    \n",
    "    # loop over all images\n",
    "    for idxA in range(len(images)):\n",
    "        # grab the current image and label belonging to the current\n",
    "        # iteration\n",
    "        current_image = images[idxA]\n",
    "        label = labels[idxA]\n",
    "        \n",
    "        # randomly pick an image that belongs to the *same* class\n",
    "        # label\n",
    "        idxB = np.random.choice(idx[label])\n",
    "        pos_image = images[idxB]\n",
    "        \n",
    "        # prepare a positive pair and update the images and labels\n",
    "        # lists, respectively\n",
    "        pair_images.append([current_image, pos_image])\n",
    "        pair_labels.append([1])\n",
    "        \n",
    "        # grab the indices for each of the class labels *not* equal to\n",
    "        # the current label and randomly pick an image corresponding\n",
    "        # to a label *not* equal to the current label\n",
    "        negIdx = np.where(labels != label)[0]\n",
    "        neg_image = images[np.random.choice(negIdx)]\n",
    "        \n",
    "        # prepare a negative pair of images and update our lists\n",
    "        pair_images.append([current_image, neg_image])\n",
    "        pair_labels.append([0])\n",
    "        \n",
    "    # return a 2-tuple of our image pairs and labels\n",
    "    return (np.array(pair_images), np.array(pair_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd063a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair_images : (304, 2, 300, 300, 3)\n",
      "pair_labels : (304, 1)\n"
     ]
    }
   ],
   "source": [
    "pair_images, pair_labels = make_pairs(img_array, label_array)\n",
    "print(f'pair_images : {pair_images.shape}')\n",
    "print(f'pair_labels : {pair_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379eab1",
   "metadata": {},
   "source": [
    "### Build triplet images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8af8f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapFunction():\n",
    "    def __init__(self, imageSize):\n",
    "        # define the image width and height\n",
    "        self.imageSize = imageSize\n",
    "    def decode_and_resize(self, imagePath):\n",
    "        # read and decode the image path\n",
    "        image = tf.io.read_file(imagePath)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        # convert the image data type from uint8 to float32 and then resize\n",
    "        # the image to the set image size\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        image = tf.image.resize(image, self.imageSize)\n",
    "        # return the image\n",
    "        return image\n",
    "    def __call__(self, anchor, positive, negative):\n",
    "        anchor = self.decode_and_resize(anchor)\n",
    "        positive = self.decode_and_resize(positive)\n",
    "        negative = self.decode_and_resize(negative)\n",
    "        # return the anchor, positive and negative processed images\n",
    "        return (anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b39fe73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletGenerator:\n",
    "    def __init__(self, datasetPath):\n",
    "        # create an empty list which will contain the subdirectory\n",
    "        # names of the `dataset` directory with more than one image\n",
    "        # in it\n",
    "        self.peopleNames = list()\n",
    "        # iterate over the subdirectories in the dataset directory\n",
    "        for folderName in os.listdir(datasetPath):\n",
    "            # build the subdirectory name\n",
    "            absoluteFolderName = os.path.join(datasetPath, folderName)\n",
    "            # get the number of images in the subdirectory\n",
    "            numImages = len(os.listdir(absoluteFolderName))\n",
    "            # if the number of images in the current subdirectory\n",
    "            # is more than one, append into the `peopleNames` list\n",
    "            if numImages > 1:\n",
    "                self.peopleNames.append(absoluteFolderName)\n",
    "        # create a dictionary of people name to their image names\n",
    "        self.allPeople = self.generate_all_people_dict()\n",
    "    def generate_all_people_dict(self):\n",
    "        # create an empty dictionary that will be populated with\n",
    "        # directory names as keys and image names as values\n",
    "        allPeople = dict()\n",
    "        # iterate over all the directory names with more than one\n",
    "        # image in it\n",
    "        for personName in self.peopleNames:\n",
    "            # get all the image names in the current directory\n",
    "            imageNames = os.listdir(personName)\n",
    "            # build the image paths and populate the dictionary\n",
    "            personPhotos = [\n",
    "                os.path.join(personName, imageName) for imageName in imageNames\n",
    "            ]\n",
    "            allPeople[personName] = personPhotos\n",
    "        # return the dictionary\n",
    "        return allPeople\n",
    "    def get_next_element(self):\n",
    "        # create an infinite generator\n",
    "        while True:\n",
    "            # draw a person at random which will be our anchor and\n",
    "            # positive person\n",
    "            anchorName = random.choice(self.peopleNames)\n",
    "            # copy the list of people names and remove the anchor\n",
    "            # from the list\n",
    "            temporaryNames = self.peopleNames.copy()\n",
    "            temporaryNames.remove(anchorName)\n",
    "            # draw a person at random from the list of people without\n",
    "            # the anchor, which will act as our negative sample\n",
    "            negativeName = random.choice(temporaryNames)\n",
    "            # draw two images from the anchor folder without replacement\n",
    "            (anchorPhoto, positivePhoto) = np.random.choice(\n",
    "                a=self.allPeople[anchorName],\n",
    "                size=2,\n",
    "                replace=False\n",
    "            )\n",
    "            # draw an image from the negative folder\n",
    "            negativePhoto = random.choice(self.allPeople[negativeName])\n",
    "            # yield the anchor, positive and negative photos\n",
    "            yield (anchorPhoto, positivePhoto, negativePhoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ae265726",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Lenovo\\\\Documents\\\\Github\\\\Datasets\\\\LFW\\\\lfw-deepfunneled\\\\lfw-deepfunneled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8896579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = BATCH_SIZE * 2\n",
    "# define autotune\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130d61e",
   "metadata": {},
   "source": [
    "### Create detected face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "63eb39c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaron_Eckhart\n"
     ]
    }
   ],
   "source": [
    "for person_name in os.listdir(path):\n",
    "    for img_name in os.listdir(os.path.join(path, person_name)):\n",
    "        img_path = os.listdir(os.path.join(path, person_name, img_name))\n",
    "        img = cv2.imread(img_path)\n",
    "        detector = MTCNN()\n",
    "        \n",
    "    print(person_name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d257c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fa762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16b9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34ecf9c9",
   "metadata": {},
   "source": [
    "### Create the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "55879dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data input pipeline for train and val dataset\n",
    "trainTripletGenerator = TripletGenerator(\n",
    "    datasetPath=path)\n",
    "\n",
    "valTripletGenerator = TripletGenerator(\n",
    "    datasetPath=path)\n",
    "\n",
    "trainTfDataset = tf.data.Dataset.from_generator(\n",
    "    generator=trainTripletGenerator.get_next_element,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "    )\n",
    ")\n",
    "\n",
    "valTfDataset = tf.data.Dataset.from_generator(\n",
    "    generator=valTripletGenerator.get_next_element,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9dc1d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building the train and validation `tf.data` pipeline...\n"
     ]
    }
   ],
   "source": [
    "mapFunction = MapFunction(imageSize=IMAGE_SIZE)\n",
    "\n",
    "print(\"[INFO] building the train and validation `tf.data` pipeline...\")\n",
    "trainDs = (trainTfDataset\n",
    "    .map(mapFunction)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "valDs = (valTfDataset\n",
    "    .map(mapFunction)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e70b0",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d2281f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (300, 300)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "STEPS_PER_EPOCH = 50\n",
    "VALIDATION_STEPS = 10\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "80210232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_module(imageSize):\n",
    "    # construct the input layer and pass the inputs through a\n",
    "    # pre-processing layer\n",
    "    inputs = keras.Input(imageSize + (3,))\n",
    "    x = resnet.preprocess_input(inputs)\n",
    "    \n",
    "    # fetch the pre-trained resnet 50 model and freeze the weights\n",
    "    baseCnn = keras.models.load_model(model_path)\n",
    "    baseCnn.trainable=False\n",
    "    \n",
    "    # pass the pre-processed inputs through the base cnn and get the\n",
    "    # extracted features from the inputs\n",
    "    extractedFeatures = baseCnn(x)\n",
    "    # pass the extracted features through a number of trainable layers\n",
    "    x = layers.GlobalAveragePooling2D()(extractedFeatures)\n",
    "    x = layers.Dense(units=1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(units=128)(x)\n",
    "    # build the embedding model and return it\n",
    "    embedding = keras.Model(inputs, outputs, name=\"embedding\")\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "363f171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_network(imageSize, embeddingModel):\n",
    "    # build the anchor, positive and negative input layer\n",
    "    anchorInput = keras.Input(name=\"anchor\", shape=imageSize + (3,))\n",
    "    positiveInput = keras.Input(name=\"positive\", shape=imageSize + (3,))\n",
    "    negativeInput = keras.Input(name=\"negative\", shape=imageSize + (3,))\n",
    "    # embed the anchor, positive and negative images\n",
    "    anchorEmbedding = embeddingModel(anchorInput)\n",
    "    positiveEmbedding = embeddingModel(positiveInput)\n",
    "    negativeEmbedding = embeddingModel(negativeInput)\n",
    "    # build the siamese network and return it\n",
    "    siamese_network = keras.Model(\n",
    "        inputs=[anchorInput, positiveInput, negativeInput],\n",
    "        outputs=[anchorEmbedding, positiveEmbedding, negativeEmbedding]\n",
    "    )\n",
    "    return siamese_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "69f6b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(keras.Model):\n",
    "    \n",
    "    def __init__(self, siameseNetwork, margin, lossTracker):\n",
    "        super().__init__()\n",
    "        self.siameseNetwork = siameseNetwork\n",
    "        self.margin = margin\n",
    "        self.lossTracker = lossTracker\n",
    "        \n",
    "    def _compute_distance(self, inputs):\n",
    "        (anchor, positive, negative) = inputs\n",
    "        # embed the images using the siamese network\n",
    "        embeddings = self.siameseNetwork((anchor, positive, negative))\n",
    "        anchorEmbedding = embeddings[0]\n",
    "        positiveEmbedding = embeddings[1]\n",
    "        negativeEmbedding = embeddings[2]\n",
    "        # calculate the anchor to positive and negative distance\n",
    "        apDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - positiveEmbedding), axis=-1\n",
    "        )\n",
    "        anDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - negativeEmbedding), axis=-1\n",
    "        )\n",
    "        \n",
    "        # return the distances\n",
    "        return (apDistance, anDistance)\n",
    "    \n",
    "    def _compute_loss(self, apDistance, anDistance):\n",
    "        loss = apDistance - anDistance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "        return (apDistance, anDistance)\n",
    "    \n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute the distance between the anchor and positive,\n",
    "            # negative images\n",
    "            (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "            # calculate the loss of the siamese network\n",
    "            loss = self._compute_loss(apDistance, anDistance)\n",
    "        # compute the gradients and optimize the model\n",
    "        gradients = tape.gradient(\n",
    "            loss,\n",
    "            self.siameseNetwork.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siameseNetwork.trainable_variables)\n",
    "        )\n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "    \n",
    "    def test_step(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "        # calculate the loss of the siamese network\n",
    "        loss = self._compute_loss(apDistance, anDistance)\n",
    "        \n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.lossTracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a705b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# build the embedding module and the siamese network\n",
    "\n",
    "embeddingModule = get_embedding_module(imageSize=IMAGE_SIZE)\n",
    "\n",
    "siameseNetwork =  get_siamese_network(\n",
    "    imageSize=IMAGE_SIZE,\n",
    "    embeddingModel=embeddingModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fa9b801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create siamese model instance\n",
    "siameseModel = SiameseModel(\n",
    "    siameseNetwork=siameseNetwork,\n",
    "    margin=0.5,\n",
    "    lossTracker=keras.metrics.Mean(name=\"loss\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0df451c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "siameseModel.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.0001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate the siamese model\n",
    "\n",
    "siameseModel.fit(\n",
    "\ttrainDs,\n",
    "\tsteps_per_epoch=config.STEPS_PER_EPOCH,\n",
    "\tvalidation_data=valDs,\n",
    "\tvalidation_steps=config.VALIDATION_STEPS,\n",
    "\tepochs=config.EPOCHS,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
